{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import sys\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'stopwords'])\n",
    "\n",
    "import sqlalchemy as sqla\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(database_filepath):\n",
    "    '''\n",
    "    INPUT:  \n",
    "        database_filepath (str): database with table name \"Messages\" having processed messages\n",
    "    OUTPUT: \n",
    "        X (pandas dataframe): messages column\n",
    "        Y (pandas dataframe): category columns marked as 1 if the message belongs to that category \n",
    "        category_names (list of strings): list of category names\n",
    "    DESCRIPTION:\n",
    "            read table named \"Messages\" from the given database\n",
    "            and select 'message' as X and all ccategories columns as Y\n",
    "            and get list of catefories as category_names\n",
    "    '''\n",
    "\n",
    "    engine = sqla.create_engine('sqlite:///'+database_filepath)\n",
    "    df = pd.read_sql('SELECT * FROM DisasterMessages', engine)\n",
    "    X = df['message']\n",
    "    Y = df.iloc[:,4:]\n",
    "\n",
    "    category_names = Y.columns.values\n",
    "\n",
    "    return X, Y, category_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "        The function is to process the sentence, token the words and lower it.\n",
    "        arg: str text\n",
    "        return:list\n",
    "        '''\n",
    "    # normalize case and remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "\n",
    "    # tokenize text\n",
    "    word_list = word_tokenize(text)\n",
    "\n",
    "    # remove stop words\n",
    "    tokens = [w for w in word_list if w not in stopwords.words(\"english\")]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    '''\n",
    "        The function is to build a pipeline and using gridsearch to training model.\n",
    "        The pipeline including countVectorizer, TfidfTransformer to process the text and using\n",
    "        RandomForestClassifier to fit the dataset\n",
    "    '''\n",
    "\n",
    "    # create ML pipeline\n",
    "    pipeline = Pipeline([('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', MultiOutputClassifier(RandomForestClassifier()))])\n",
    "\n",
    "    # specify parameters for grid search\n",
    "    ''' \n",
    "       parameters = {\n",
    "            'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "            'vect__max_df': (0.5, 0.75, 1.0),\n",
    "            'vect__max_features': (None, 5000, 10000),\n",
    "            'tfidf__use_idf': (True, False),\n",
    "            'clf__n_estimators': [50, 100, 200],\n",
    "            'clf__min_samples_split': [2, 3, 4]\n",
    "        }\n",
    "    '''\n",
    "    parameters = {'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "                 'tfidf__use_idf': (True, False),\n",
    "                 'clf__estimator__n_estimators':[50, 100, 200],\n",
    "                 'clf__estimator__max_depth':[50, 500, 1000, 5000],\n",
    "                 'clf__estimator__max_features': [2000, 5000, 10000, 20000],\n",
    "                 'clf__estimator__min_samples_split':[3, 5, 9]}\n",
    "    model = GridSearchCV(pipeline, param_grid=parameters)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, Y_test, category_names):\n",
    "    '''\n",
    "        The function is to return the results of prediction on test dataset, including precision socre,\n",
    "        f1-score and recall score.\n",
    "        args: model, test dataset and category names\n",
    "        return: dict - the classification report of category names\n",
    "    '''\n",
    "\n",
    "    y_pred = model.predict(X_test)  # prediction\n",
    "    prediction = pd.DataFrame(y_pred.reshape(-1, 36), columns=category_names)  # transform list to dataframe\n",
    "    report = dict()\n",
    "    for i in category_names:\n",
    "        # iterate the category names and add its classification scores to dictionary\n",
    "        classification = classification_report(Y_test[i], prediction[i])\n",
    "        report[i] = classification\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_filepath):\n",
    "    '''\n",
    "       INPUT:\n",
    "           model (str): trained model\n",
    "           model_filepath (str): pickle file path to save the model\n",
    "       OUTPUT:\n",
    "       DESCRIPTION:\n",
    "               save the model passed as the path given as input\n",
    "       '''\n",
    "\n",
    "    pickle.dump(model, open(model_filepath, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X, Y, category_names = load_data('DisasterResponse.db')\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "category_names\n",
    "\n",
    "print('Building model...')\n",
    "model = build_model()\n",
    "\n",
    "print('Training model...')\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "print('Evaluating model...')\n",
    "evaluate_model(model, X_test, Y_test, category_names)\n",
    "\n",
    "print('Saving model...\\n ')\n",
    "save_model(model, model_filepath)\n",
    "\n",
    "print('Trained model saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
